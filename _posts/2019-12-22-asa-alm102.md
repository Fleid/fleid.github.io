---
layout: post
title:  "Basic ALM setup for Azure Stream Analytics, Part 2 - ALM 102 for ASA"
date:   2019-12-22 10:00:00 -0700
categories: ALM Azure ASA DevOps
---

# Basic ALM setup for Azure Stream Analytics,Part 2 - ASA-ALM-102

This is the **second article** of a series on enabling modern ALM practices for an Azure Stream Analytics project. Here's the [first one](https://www.eiden.ca/asa-alm-101/).

## Context

We are [continuing the work](https://www.eiden.ca/asa-alm-101/) of enabling modern ALM practices for our ASA project.

This time we will start with building a live streaming job, so we give ourselves the opportunity to feel the pain of deploying running jobs. Once this is done, we will later be able to set up a build pipeline (generate and validate ARM templates), followed by a release pipeline (execute ARM deployment).

The ASA job we will use is the one offered in the [quick-start tutorial](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal) of the official documentation. Events are generated in the [Raspberry Pi Azure IoT Online simulator](https://azure-samples.github.io/raspberry-pi-web-simulator/) (it's a simulator, no need for a physical device), pushed to an IoT Hub, ingested by the ASA job and output to a blob store.

![Illustration of our pipeline](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_alm102_pipe.png?raw=true)

Let's get started!

## Live data source

The first part to build is the data source. We should not consider that data source as part of our project. It could be used by another ASA job, or any other consumer application. That's why we'll put it in a separate resource group: ```rg-shared```.

- In the [Azure portal](https://portal.azure.com), let's create a new resource group to hold our shared assets: ```rg-shared```
- We will create the IoT Hub following [the steps 1 to 8](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal#prepare-the-input-data) replacing the resource group by our ```rg-shared```
- Then we can start the IoT simulator by following [this step](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal#run-the-iot-simulator). The IoT hub will store these events for us while we setup our job.

Once this is done, we can start provisioning our project resources.

## Provisioning

We will not use the portal to provision our resource group, ASA job and storage account. Instead we will using both the **Azure CLI** and **PowerShell**.
Why scripting? Because if we're to do modern ALM, we should thrive to never use graphical interfaces. Every steps we take, we should be able to automate. Why two tools? We would only use the Az CLI if we could. In my opinion it's simpler and more practical. The thing is that at the time of writing it doesn't have an option to manage ASA jobs. That's what we will use PowerShell for.

It is to be noted that it is usually a pain to get PowerShell happy, and in the right version.

- For the **Azure CLI**, the [install procedure](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest) is straightforward, particularly on Windows where there's [an executable](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-windows?view=azure-cli-latest)
    - Once installed, do run the ```az login``` command in a terminal (cmd) to connect your environment to your Azure subscription
    - The environment under which the Az CLI scripts are usually ran is [bash](https://en.wikipedia.org/wiki/Bash_%28Unix_shell%29), since the CLI itself is cross-platform. Windows 10 offers a bash experience via the WSL (Windows Subsystem for Linux) available after [a quick install](https://docs.microsoft.com/en-us/windows/wsl/install-win10). I highly recommend going through those steps and starting to use the WSL, but if it's not possible, the walk-around is to do everything in PowerShell instead (or in the portal directly, but no scripting then)
- For **PowerShell**, two steps:
    - While not mandatory, it's better (and actually the only option cross-platform) to [install version 6.x or above](https://docs.microsoft.com/en-us/powershell/scripting/install/installing-powershell-core-on-windows?view=powershell-6). It's an awkward dance of going to the [release page](https://github.com/PowerShell/PowerShell/releases), finding a version we like (usually the latest neither preview or release candidate, [v6.2.3](https://github.com/PowerShell/PowerShell/releases/tag/v6.2.3) at the time of writing), looking under assets and finding the right version (...-win-x64.msi for Windows folks)
    - Once this is done, we will need to install the **Azure PowerShell Az** module, which [should be easy](https://docs.microsoft.com/en-us/powershell/azure/install-az-ps?view=azps-3.2.0)... well, if you hadn't already installed the **Azure PowerShell AzureRM** module. In which case you will need that PowerShell version 6.x or above, since AzureRM can't live there, or to [uninstall AzureRM](https://docs.microsoft.com/en-us/powershell/azure/uninstall-az-ps?view=azps-3.2.0#uninstall-the-azurerm-module) from your existing environment (but what about your existing scripts?)
        - Let this be a good reminder that we will often find commands for **AzureRM** (older. so more docs and blog posts about it) that won't work as is in **Az**. But usually an equivalent command exists, we just need to find them [in the doc](https://docs.microsoft.com/en-us/powershell/module/?view=azps-3.2.0). Why there is 2 flavors of Azure PowerShell is [explained here](https://docs.microsoft.com/en-us/powershell/azure/new-azureps-module-az?view=azps-3.2.0).
        - If something misbehaves, [have you tried turning it on and off again](https://www.youtube.com/watch?v=nn2FB1P_Mn8)?
        - I sometimes have issues where VSCode get lost in the version of PowerShell to use in its terminal. Restarting the app usually solves that. Also be sure to use the **PowerShell Integrated Console** in its terminal and check that the bottom right icon shows the 6.x version it should display. It's in that environment that you want the Az module to be installed
        - I once got ```Your Azure credentials have not been set up or have expired, please run Connect-AzAccount to set up your Azure credentials``` error message despite just having used ```Connect-AzAccount```, I used ```Remove-AzAccount``` to remove the account and start again

Hopefully this didn't take a whole afternoon, so we still have time to use these tools.

In **VSCode**, in our project, we can create a new folder named ```Provision```. In that folder, let's add our first file: ```provision01.azcli```. The fact we named it ```.azcli``` made it an Azure CLI script, which should get **VSCode** to chime in about an extension available for it (we can use [that](https://marketplace.visualstudio.com/items?itemName=ms-vscode.azurecli) if it didn't). Now we're using the same IDE to write our provisioning scripts, and version control those scripts in the same repository as the project. Neat.

Let's write that script. Note that we can run all or part of it in the VSCode terminal by using ```CTRL+'```:

```Bash
# Using Bash (WSL if Windows)
bash

# Login to Azure
az login

# If necessary (multiple subscriptions available), select the appropriate one
az account list --output table
az account set --subscription "mySubscriptionNameGoesHere"

# Set variables (bash syntax)
_location="canadacentral"
_suffix="staging"
_datecreated=`date +"%Y%m%d"`

_rg_name="rg-asatest"
_rg_name+=$_suffix
_sa_name="saasatest"
_sa_name+=$_suffix
_sa_container="container001"

# Create a resource group
az group create --location $_location  --name $_rg_name --tags 'createdby=fleide' 'datecreated='$_datecreated 'environment=sta' 'purpose=stream-processing'

# Create a storage account
az storage account create \
    --name $_sa_name \
    --resource-group $_rg_name \
    --location $_location \
    --sku Standard_LRS \
    --encryption blob \
    --kind StorageV2

# Get the key of that storage account to create a storage container later
_key1=$(az storage account keys list -g $_rg_name -n $_sa_name --query '[0].value' -o tsv)

# Create a storage container
az storage container create --account-name $_sa_name --account-key $_key1 --name $_sa_container
```

Executing that script should give us a resource group ```rg-asateststaging```, inside of which we'll find a storage account ```saasateststaging```, itself host of a container ```container001```. We can always use the [Azure portal](https://portal.azure.com) or the [Azure Storage Explorer](https://azure.microsoft.com/en-us/features/storage-explorer/) to check for ourselves.

To provision an ASA job using PowerShell, we will use the ```New-AzStreamAnalyticsJob``` command. According to the [documentation](https://docs.microsoft.com/en-us/powershell/module/az.streamanalytics/new-azstreamanalyticsjob?view=azps-3.2.0), it requires a job definition file to do so. We know what that is: the ARM template file we created [earlier](https://www.eiden.ca/asa-alm-101/). But let's be more generic than that, and create a minimal definition file, adding a new ```JobTemplate.provision.json``` file to our ```Provision``` folder with the following content:

```JSON
{
    "location":"CanadaCentral",
    "properties":{
      "sku":{
        "name":"standard"
      },
      "eventsOutOfOrderPolicy":"adjust",
      "eventsOutOfOrderMaxDelayInSeconds":10,
      "compatibilityLevel": 1.1
    }
  }
```

The final piece is the ASA job itself. Again, since the Az CLI can't help us, we'll add a second file ```provision02.ps1``` to our ```Provision``` folder. This time the ```.ps1``` file extension should trigger **VSCode** to mention its [PowerShell extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode.PowerShell). This time we can run selection using ```F8```:

```PowerShell
# Login to Azure
Connect-AzAccount

# If necessary (multiple subscriptions available), select the appropriate one
Get-AzSubscription
Get-AzSubscription -SubscriptionName "mySubscriptionNameGoesHere" | Select-AzSubscription

# Create a Stream Analytics job
$suffix = "staging"

$rg_name = "rg-asatest$suffix"
$jobName = "MyStreamingJob$suffix"

$currentLocation = Get-Location
$jobDefinitionFile = "$currentLocation\Provision\JobTemplate.provision.json"

New-AzStreamAnalyticsJob `
  -ResourceGroupName $rg_name `
  -File $jobDefinitionFile `
  -Name $jobName `
  -Force
```

Checking in the portal, we should also now have an empty Streaming Analytics job in our resource group:

![Content of our RG](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_alm102_rg.png?raw=true)

Of course we committed and pushed all of these changes regularly to our origin repo... but if we hadn't, now would be a good time.

## ASA Job

Now that we have a live input and a live output, let's wire them in our Stream Analytics project. In VSCode:

- For the live input, from the tutorial, [steps 1 to 5](https://docs.microsoft.com/en-us/azure/stream-analytics/quick-create-vs-code#define-a-live-input)
- For the live output, from the tutorial, [steps 1 to 5](https://docs.microsoft.com/en-us/azure/stream-analytics/quick-create-vs-code#define-an-output)

Finally we can edit our query (replacing the names of input/output by the name we defined):

```SQL
SELECT *
INTO [BlobStorage1]
FROM [IoTHub1]
HAVING Temperature > 27
```

And test a local execution (```Run locally```) on a live input (remembering to start the [simulator](https://azure-samples.github.io/raspberry-pi-web-simulator/) if need be):

![Illustration of a local live run](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_alm102_locallive.png?raw=true)

Now that this is done (and after doing a commit/push), we can deploy that job to Azure. In the next article, we will see how to use Azure Pipelines to generate the ARM template files to do so, but for now we can just use the VSCode command ```ASA: Submit to Azure``` to do so (```CTRL+SHIT+P``` or top of editor with query open). Of course we will select the job we just created, and **publish to Azure**.

If we want to see the job running in Azure (and check output files in our target storage account), we can start the job from the Azure portal or in VSCode (Azure tab on the left, finding the job in the subscription and right-clicking to start).

After a short wait, the job should be running:

![Illustration of a cloud run](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_alm102_locallive.png?raw=true)

**REMEMBER TO STOP THE JOB** if you don't need it. It's a piece of compute that runs 24/7, so even if it's cheap it will still eat our budget.
The rest of the resources, including the shared ones, are close to free, so no real worries there. Worst case they can be deleted, since we have the scripts to rebuild them on demand.

## Next steps

- The release pipeline, from compiling ASA assets into an ARM template to publishing that to the service via Azure Pipelines
- Adding a Cosmos DB output to our data pipeline to debug a live stream
