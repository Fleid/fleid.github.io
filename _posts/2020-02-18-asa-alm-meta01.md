---
layout: post
title:  "Backlog update on the Azure Stream Analytics ALM Series"
date:   2020-02-18 10:00:00 -0700
categories: ALM Azure ASA DevOps Product Planning Backlog
---

# Backlog update on the Azure Stream Analytics ALM Series #1

When I look at [my blog](https://www.eiden.ca/) recently, I feel like I haven’t made any progress on the [Azure Stream Analytics ALM series](https://www.eiden.ca/asa-alm-100/). To be fair to myself, there is a lot I need to prepare before I can  move on to writing the next post, and it’s taking more time than I expected (to nobody’s surprise except me). I wanted to take the time to share what’s keeping me busy.

The next article will cover **unit-testing**, both local and running in the build phase of the CI pipeline. The main difficulty on this topic is that, at the moment, unit testing is not supported natively for ASA in either VSCode or Visual Studio. The good news is that the [Visual Studio ASA extension](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-tools-for-visual-studio-cicd) provides building blocks that can be wired together to offer the basics of it.

Since I was having a ton of fun with [PowerShell](https://www.eiden.ca/tag/powershell/) lately, I decided to do just that, and hacked a solution that I published on [GitHub](https://github.com/Fleid/asa.unittest). It’s basically a PowerShell script that leverages a fixture to run tests programmatically. It’s a bit hacky, but it does its thing. As I was polishing it and adding a couple of features, I realized that if it was a very good prototype, **it was not production ready at all**. Ironically it was missing unit-testing and a proper release pipeline. I couldn’t let that fly.

To unit-test a PowerShell script it needs to be designed and built the right way: the way its authors designed it to be. So I took the time to sit down and [learn PowerShell](https://www.manning.com/books/learn-windows-powershell-in-a-month-of-lunches-third-edition) properly. It helped me rewrite the script in a much [cleaner way](https://github.com/Fleid/asa.unittest/blob/master/unittest/2_act/Start-AutRun.ps1). Then I tried to unit-test it with [Pester](https://github.com/pester/Pester), but if the internals were clean, the modularity of it (interface and flow of operations) was still wrong. It was awkward to test anything, and I had to mock way too many things to get any results.

My script was just that, a script, that did everything in one single file (class?). So I went back in research mode and read the [PowerShell Toolmaking book](https://www.manning.com/books/learn-powershell-scripting-in-a-month-of-lunches), and it helped a lot. What also helped was that I went through [A Philosophy of Software Design](https://www.goodreads.com/book/show/39996759-a-philosophy-of-software-design) at the same time, which was reiterating the same ideas from a more agnostic point of view.

Now I needed to re-design my solution with the following constraints in mind:

- One **controller** script calling multiple **tool** scripts
  - The **controller** script is the user interface
    - It needs to be narrow and deep
    - It’s about context, it knows about ASA and what we’re trying to accomplish
  - The **tool** scripts do only one thing each
    - No context, no hard-coded values, as generic and reusable as possible
    - Use verb-noun naming convention and only get data via parameter binding
    - Are testable because self-contained

The way to design either forms of script is to:

1. Map business requirements via example calls with parameters
2. Derive its unique signature from those calls + the constraints above
3. Build it

I tried that for a single tool script I had already build (`New-AutAsaprojXML`) and it was weird but good. That tool converts a JSON autoproj (config file generated by the ASA extension for VSCode) into an equivalent XML one (generated by Visual Studio, expected for a scripted run). It’s awkward because it still carries context – I don’t want to build a generic JSON to XML converter, so it knows about the asaproj details – but I remodeled it enough to be testable (object as input parameter, string as output parameter, no reading/writing files). And it’s now unit-tested, and it’s awesome. I already caught a couple of bugs thanks to it.

Now I must refactor the existing tool scripts, and extract functions/tools from the controller script so I can test most of it independently. But that opens another box: **how am I supposed to structure my solution** (as in files and folders) to be easy to develop, test and release. PowerShell offers [a lot of options](https://powershellexplained.com/2017-05-27-Powershell-module-building-basics/) on how to package scripts (functions, modules…), choosing one is not straightforward for a beginner.

Also **testing is just the first part of the equation**. If I apply the design process to the controller script, thinking about how I want my end users to experience the solution, then I don’t want them to have to run multiple scripts, in specific sub-folders, scripts they got from a unknown GitHub repo… The best way I can see them use it is by going:

```PowerShell
Import-Module Asa.Unittest
New-AutFixture -Path “C:\...”
Start-AutRun -Path “C:\...” -AsaPath “C:\...”
```

That would be awesome! But that means publishing the tool in the [PowerShell gallery](https://www.powershellgallery.com/), which comes with its own [set of guidelines and best practices](https://docs.microsoft.com/en-us/powershell/scripting/gallery/concepts/publishing-guidelines?view=powershell-7). Which also means a strong release pipeline, [a project in itself](https://powershellexplained.com/2017-01-21-powershell-module-continious-delivery-pipeline/?utm_source=blog&utm_medium=blog&utm_content=tags).

All that for no visible feature shipped. The whole story of software planning and delivery unfolding at the scope of my little product!

So here I am, with a quick status update on that ALM series, sharing what’s in my backlog, learning a ton and having fun!
