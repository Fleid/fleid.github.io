---
layout: post
title:  "Basic ALM setup for Azure Stream Analytics - ALM 101 for ASA"
date:   2019-12-06 10:00:00 -0700
categories: ALM Azure ASA DevOps
---

# Basic ALM setup for Azure Stream Analytics - ASA-ALM-101

This is the first article of a series on enabling modern ALM practices for an Azure Stream Analytics project.

This is a work in progress, assumptions are being made, use at your own risk ;)

## Context

> Data streaming is awesome. Azure is awesome. Let's build a real time data pipeline using [Azure Stream Analytics](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-introduction) then! And now with the cloud and DevOps, let's do things right and "*Infrastructure as Code*" and "*CI/CD*" all the things!

Or so I told myself before struggling with implementation details so frustrating, I almost quit data engineering entirely. I reassured myself with the thought that this pain is often good evidence that work is being done on something that matters. Right?

To really benefit from what follows, one should have already played a little bit with **Azure Stream Analytics** (ASA). I expect it to be the case [directly in the Azure portal](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal). This article focus on moving out of the portal, and considering the [ALM](https://en.wikipedia.org/wiki/Application_lifecycle_management) (Application Life cycle Development) aspects of that transition. There is nothing below about the use case / scenario, the query or the pipeline itself.

### Pipelines?

As for everything ["DataOps"](https://medium.com/data-ops/why-do-dataops-8d4542eec3e5), we're going to have to consider two pipelines (at least). On one side, there's the data pipeline: the real time one that ASA enables. On the other side, there's the development pipeline: the meta one that takes your code from your IDE through staging and eventually production.

This schema from [DataKitchen](https://medium.com/data-ops) shows how data is moving left to right, and code is moving bottom to top:

![Two pipelines: data and code](https://miro.medium.com/proxy/1*Oo_SUpo729y9iuW14tUFwQ.png)

**Azure Stream Analytics** is the main engine of the **streaming pipeline**.

[Azure DevOps](https://dev.azure.com) is the main engine of the **development pipeline**, both with [Azure Repos](https://azure.microsoft.com/en-us/services/devops/repos/) and [Azure Pipelines](https://azure.microsoft.com/en-us/services/devops/pipelines/).

The plan is to set up a **local development environment** disconnected from Azure. There we'll create an ASA project and write a simple data pipeline that reads data from a sample input, run a basic query locally and output that somewhere. We will check that project in Azure Repos using Git. From there we'll setup a build (we'll see what that means in our context) and release pipeline to a staging environment.

Note that if you're looking for tips on how to test ASA jobs, I should do a first pass on unit testing at some point, and share some thoughts on my plans for integration testings. This won't be a deep coverage though, that may happen later.

### Components

Taking all that into account, we can start to assemble our puzzle:

![Schema](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_alm099.png?raw=true)

Of course an actual release pipeline will be more complex than a simple local development to staging flow. But if we get the basic wiring right, we can add as many intermediary steps or targets as necessary (staging, integration, UAT, pre-prod, prod...).

Now if we want to do proper local development, we expect to be able to do local runs. This will be enabled by ASA Extensions available for [VSCode](https://docs.microsoft.com/en-us/azure/stream-analytics/quick-create-vs-code) and [Visual Studio](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-vs). The second required feature to be really disconnected is the ability to locally run on locals input. This is also available in [VSCode](https://docs.microsoft.com/en-us/azure/stream-analytics/vscode-local-run) and [Visual Studio](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-vs-tools-local-run) using "sample data".

Let's focus on a single IDE here, and pick [VSCode](https://code.visualstudio.com/). The main reason is that I've been enjoying it lately, using it [remotely on the WSL](https://www.hanselman.com/blog/VisualStudioCodeRemoteDevelopmentMayChangeEverything.aspx), so why not.

The last thing we need to be aware of is that ASA jobs are deployed via [ARM templates](https://docs.microsoft.com/en-us/azure/azure-resource-manager/). ARM Templates are JSON files that describe an Azure resource, here an ASA job. You usually get a generic one, the template, and one that holds credentials and values that should be parameterized.

Let's re-organize that schema and go a bit deeper in each areas - and yes, I'm spoiling everything. Please note that this is not the end state, quite the opposite it's the first, most basic step towards a solid delivery pipeline of an ASA project.

![Schema](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_alm100.png?raw=true)

In the rest of the article we'll discuss:

- The data pipeline itself, with the Raspberry generator and IoT Hub, using Cosmos DB as an live debug output, and provisioning the different resource groups.
- The developer environment setup, with Git, VSCode and its ASA extension. It's that last one that will enable local runs.
- The entire release pipeline from compiling ASA assets into an ARM template in VSCode to publishing that to the service via Azure Pipelines

Let's dig in.

## Developer setup

Let's build that developer environment, independent from Azure.

## Data Pipeline

### Input

To have fun in a streaming scenario, we need a streaming data source. It used to be painful to get that part set up, now it's trivial.

One of the easiest one is to leverage the [Raspberry Pi Azure IoT Online Simulator](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal#run-the-iot-simulator) via an [IoT Hub](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal#prepare-the-input-data). To my knowledge it needs to sit in an open browser tab, so another option is to build a quick Azure Function [triggered by a timer](https://docs.microsoft.com/en-us/azure/azure-functions/functions-create-scheduled-function), bound to an [Event Hub in output](https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-event-hubs#output). I should cover that in another article later, but the coding is [straightforward](https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-event-hubs#output---c-example).

That IoT Hub (or Event Hub) will **not** be part of our project. In a real life scenario, it's either provided by another service, or needs to be put in a separate project since its dependencies and its life cycle rhythm are both different than our main job.

### Output

This basic pipeline will get a basic primary output: a blob store. It's fitting since it doesn't care about schema drift (crafting a job against SQL is always a pain since it requires a table update every time the schema is updated. We can push data in any format, and we don't really care about reading it since we won't really do that anyway.

I'm currently experimenting with using Cosmos DB with container set up with a short TTL (time-to-live) to observe what's flying in the pipeline. It's also schema-on-read, it can scale with ASA, allows exactly-once delivery (more on that later), and provide a Data Explorer experience form the portal. It's not free, but it's cheap for the value it brings.

### Query

Here is the query I'm currently using, notice 2 patterns: audit / prod-flag to debug output.