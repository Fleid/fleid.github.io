---
layout: post
title:  "Basic ALM setup for Azure Stream Analytics - ALM 101 for ASA"
date:   2019-12-06 10:00:00 -0700
categories: ALM Azure ASA DevOps
---

# Basic ALM setup for Azure Stream Analytics - ALM 101 for ASA

## Context

> Data streaming is awesome. Azure is awesome. Let's build a real time data pipeline using [Azure Stream Analytics](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-introduction) then! And now with the cloud and DevOps, let's do things right and "*Infrastructure as Code*" and "*CI/CD*" all the things!

Or so I told myself before struggling with implementation details so frustrating, that I almost quit data engineering entirely. I reassured myself with the thought that this pain is often good evidence that the work is done on something that matters. Right? 

Let's move on.

To really benefit from what follows, one should have already played a little bit with **Azure Stream Analytics** (ASA). I expect it to be the case [directly in the Azure portal](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal). This article focus on moving out of the portal, and considering the [ALM](https://en.wikipedia.org/wiki/Application_lifecycle_management) (Application Lifecyle Development) aspects of that transition. There is nothing below about the use case, the query or the pipeline itself.

### Pipelines?

As for everything ["DataOps"](https://medium.com/data-ops/why-do-dataops-8d4542eec3e5), we're going to have to consider two pipelines (at least). On one side, there's the data pipeline: the real time one that ASA enables. On the other side, there's the development pipeline: the meta one that takes your code from your IDE through staging and eventually production.

This schema from [DataKitchen](https://medium.com/data-ops) shows how data is moving left to right, and code is moving bottom to top:

![Two pipelines: data and code](https://miro.medium.com/proxy/1*Oo_SUpo729y9iuW14tUFwQ.png)

**Azure Stream Analytics** is the main engine of the **streaming pipeline**. [Azure DevOps](https://dev.azure.com) is the main engine of the **development pipeline**, both with [Azure Repos](https://azure.microsoft.com/en-us/services/devops/repos/) and [Azure Pipelines](https://azure.microsoft.com/en-us/services/devops/pipelines/).

The plan is to set up a **local development environment** disconnected from Azure. There we'll create an ASA project and write a simple data pipeline that reads data from a sample input, do a basic query in ASA and output that somewhere. We will check that project in Azure Repos using Git. From there we'll setup a build (we'll see what that means in our context) and release pipeline to a staging environment. 

Note that if you're looking for tips on how to test ASA jobs, I'll do a first pass on unit testing here (far from perfect), and share some thoughts on my plans for integration testings. This is not enough though, but I intend to cover the topic more deeply in another article.

Taking all that into account, we can start to assemble our puzzle:

![Schema](\201912_asa_alm101\asa_alm099.png)

Of course an actual release pipeline will be more complex than local development to staging, but if we get the basic wiring right, we can add as many intermediary steps/targets as necessary (staging, integration, UAT, pre-prod, prod...).

Let's re-organize that schema and go a bit deeper in each areas - and yes, I'm spoiling everything. Please note that this is not the end state, quite the opposite it's the first, most basic step towards a solid delivery pipeline of an ASA project.

![Schema](\201912_asa_alm101\asa_alm100.png)

In the rest of the article we'll discuss:

- The data pipeline itself, with the Raspberry generator and IoT Hub, using Cosmos DB as an live debug output, and provisioning the different resource groups.
- The developer environment setup, with Git, VSCode and its ASA extention. It's that last one that will enable local runs.
- The entire release pipeline from compiling ASA assets into an ARM template in VSCode to publishing that to the service via Azure Pipelines

Let's dig in.
