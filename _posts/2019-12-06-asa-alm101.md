---
layout: post
title:  "Basic ALM setup for Azure Stream Analytics - ALM 101 for ASA"
date:   2019-12-06 10:00:00 -0700
categories: ALM Azure ASA DevOps
---

# Basic ALM setup for Azure Stream Analytics - ASA-ALM-101

This is the first article of a series on enabling modern ALM practices for an Azure Stream Analytics project.

This is a work in progress, assumptions are being made, use at your own risk ;)

## Context

> Data streaming is awesome. Azure is awesome. Let's build a real time data pipeline using [Azure Stream Analytics](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-introduction) then! And now with the cloud and DevOps, let's do things right and "*Infrastructure as Code*" and "*CI/CD*" all the things!

Or so I told myself before struggling with implementation details so frustrating, I almost quit data engineering entirely. I reassured myself with the thought that this pain is often good evidence that work is being done on something that matters. Right?

To really benefit from what follows, one should have already played a little bit with **Azure Stream Analytics** (ASA). I expect it to be the case [directly in the Azure portal](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal). This article focus on moving out of the portal, and considering the [ALM](https://en.wikipedia.org/wiki/Application_lifecycle_management) (Application Life cycle Development) aspects of that transition. There is nothing below about the use case / scenario, the query or the pipeline itself.

### Pipelines?

As for everything ["DataOps"](https://medium.com/data-ops/why-do-dataops-8d4542eec3e5), we're going to have to consider two pipelines (at least). On one side, there's the data pipeline: the real time one that ASA enables. On the other side, there's the development pipeline: the meta one that takes our code from our [IDE](https://en.wikipedia.org/wiki/Integrated_development_environment) through staging and eventually production.

This schema from [DataKitchen](https://medium.com/data-ops) shows how data is moving left to right, and code is moving bottom to top:

![Two pipelines: data and code](https://miro.medium.com/proxy/1*Oo_SUpo729y9iuW14tUFwQ.png)

**Azure Stream Analytics** is the main engine of the **streaming pipeline**.

[Azure DevOps](https://dev.azure.com) is the main engine of the **development pipeline**, both with [Azure Repos](https://azure.microsoft.com/en-us/services/devops/repos/) and [Azure Pipelines](https://azure.microsoft.com/en-us/services/devops/pipelines/).

The plan is to set up a **local development environment** disconnected from Azure, either using [Visual Studio](https://visualstudio.microsoft.com/downloads/) or [Visual Studio Code](https://code.visualstudio.com/?wt.mc_id=vscom_downloads) (VSCode). There we'll create an ASA project and write a simple data pipeline that reads data from a sample input, **run a basic query locally** and output that to disk. We should also write some **scripts that can be used to provision** the infrastructure we will later need on Azure when time comes to deploy. We will check that project in **Azure Repos using Git**. From that we'll setup a **build** (we'll see what that means in our context) and **a release pipeline** to a staging environment.

Note that if you're looking for tips on how to test ASA jobs, I should do a first pass on unit testing at some point, and share some thoughts on my plans for integration testings. This won't be a deep coverage though, that may happen later hopefully.

### Components

Taking all that into account, we can start to assemble our puzzle:

![Making that schema ours](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_alm099.png?raw=true)

Of course an actual release pipeline will be more complex than a simple local development to staging flow. But if we get the basic wiring right, [we can add as many intermediary steps](https://docs.microsoft.com/en-us/azure/devops/pipelines/release/define-multistage-release-process?view=azure-devops) or targets as necessary (staging, integration, UAT, pre-prod, prod...).

Now if we want to do proper local development, we expect to be able to do local runs on local inputs and outputs. This will be enabled by the ASA extensions available for [VSCode](https://docs.microsoft.com/en-us/azure/stream-analytics/quick-create-vs-code) and [Visual Studio](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-vs). Using local inputs/outputs, with sample data, is a supported scenario both in [VSCode](https://docs.microsoft.com/en-us/azure/stream-analytics/vscode-local-run) and [Visual Studio](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-vs-tools-local-run).

Let's focus on a single IDE from now on, and pick [VSCode](https://code.visualstudio.com/). The main reason is that I've been enjoying it a lot lately, using it [remotely on the WSL](https://www.hanselman.com/blog/VisualStudioCodeRemoteDevelopmentMayChangeEverything.aspx). I don't expect things to be too different using Visual Studio instead.

The last thing we need to be aware of is that ASA jobs are deployed via [ARM templates](https://docs.microsoft.com/en-us/azure/azure-resource-manager/). ARM Templates are JSON files that describe a specific Azure resource, here an ASA job. ARM templates are usually 2 files: one that describe the resource itself, and one that holds credentials and values that should be parameterized. The ASA extension will handle those files for us, ["compiling"](https://docs.microsoft.com/en-us/azure/stream-analytics/quick-create-vs-code#compile-the-script) our ASA assets (query file, input/output files, config files) into the ARM template files.

Taking all that into account, at the end of our local setup we should get something like this:

![A representation of the assets discussed above](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_alm100_local.png?raw=true)

Once this is done, in the rest of the series we'll later discuss:

- The data pipeline itself running in Azure, with live input/outputs - including using Cosmos DB to debug a live stream
- The release pipeline from compiling ASA assets into an ARM template in VSCode to publishing that to the service via Azure Pipelines

Let's dig in.

## Developer setup

### Requirements

- Install [Visual Studio Code](https://code.visualstudio.com/)
- Install the Azure Stream Analytics extension
  - Open **VSCode**
  - From **Extensions** on the left pane, search for **Stream Analytics** and select **Install** on the Azure Stream Analytics extension
    - Note that there are other prerequisites but VSCode should chime in if they are not installed

![ASA Extension in VSCode screen capture](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_almASAextension.png?raw=true)

### Project

Now let's create a project.

To access the commands made available from the ASA extensions, we need to use the [command palette](https://code.visualstudio.com/docs/getstarted/userinterface#_command-palette). Two ways to do that: ```CTRL+SHIFT+P``` or via the menu : ```View > Command Palette```. Once it's there, we can type ```ASA create``` and select the ```ASA: Create new project``` option.

![ASA charm: Create new project](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_almASAproject.png?raw=true)

We'll pick a name for our project (I left the default option, **ASATest1**), then a folder, and we should end up with the following workspace:

- **ASATest1** (project folder)
  - **Functions** ([Javascript](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-javascript-user-defined-functions) or [C#](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-edge-csharp-udf) user defined functions (UDF), we should not use that one anytime soon)
  - **Inputs** (where our input configuration files will live)
    - *input.json* : an empty input configuration file to get us started
  - **Outputs** (where our output configuration files will live)
    - *output.json* : an empty output configuration file to get us started
  - *asaproj.json* : the configuration file of our project
  - *ASATest1.**asaql*** : our job query
  - *JobConfig.json* : the configuration file of the job, once it'll be running live (scale, compatibility version, error policy...)

Now that we have that, we should be able to compile these assets into ARM Templates files. From the command palette again, let's do a ```ASA Compile Script```:

![ASA charm: Create new project](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_almASAcompile.png?raw=true)

Which will add the following sub-folder and file to the project:

- **ASATest1** (project folder)
  - **Deploy**
    - *ASATest1.JobTemplate.json* : the job definition - we should never edit that file directly, but instead change the source files and compile
    - *ASATest1.JobTemplate.parameters.json* : that parameters (connection strings...) - we should never edit that file either, we will use variables in the release pipeline to manage those values properly
  - ...

This will be what we leverage later when we first set up the release pipeline: manual compile step in VSCode, then pushing those files to our repo, which will trigger a build using the ARM template files.

Speaking of our repo, we should actually set Git up before anything else - well we should have set Git up before even creating the project, but here we are.

### Version control with Git

As mentioned before, we will be using **Azure DevOps** ([free account](https://dev.azure.com)), and in this section we'll focus on **Azure Repos**.

We won't go through the details, the plan is to first create a DevOps project, then a repo for our local project. Since I tend to reserve the base repo (the default one created with the DevOps project) for documentation, we'll create a new one specifically for our ASA project:

![Azure DevOps new repo](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_almDevOpsproject.png?raw=true)

Then

![Azure DevOps new repo](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_almDevOpsrepo.png?raw=true)



### Local run

https://docs.microsoft.com/en-us/azure/stream-analytics/visual-studio-code-local-run

So the doc is not updated, but I should be able to test locally, in VS Code, both for local sample data and live stream input.

The tutorial is summary, it just tells you to create a local input, and run the query on it. Let's generate a test file from scratch then. Or I could download what was put on blob the last time the job ran. I need to install [Storage Explorer](https://azure.microsoft.com/en-us/features/storage-explorer/)
**NB**: [Info] 11/11/2019 10:14:46 PM : Warning : Time policy for stream analytics job is not supported for local static input file.

After playing a bit with the setup, I settled on the following: 

- a ```Local_myInputfile.json``` definition file
- a ```Data_myInputfile.xxx``` for the test data,
  - In this file I add a ```_testId``` column for the test case number
- in the query that needs to be tested I add ```COALESCE(_testID,0) AS Audit``` as an output column

Now I need the following:

- [x] to add the LocalRunOutputs folder to the gitignore

### Version control

I created new Azure Devops project (I already had an account there, but it's free anyway).

There I created 3 new repos: Ticker (consoleapp), Function01 and ASAtopologu01 and initialized them. I'm making assumptions here in terms of repo strategy. I want to isolate per technology by habit, we'll see how it goes. Still I'm not sure how to handle infra as code artifacts, At the moment I want each component to be self contained (Ticker repo will host deployment code for ticker resources), but then where to put shared assets? I'll surely need a shared infra repo but let's feel the pain before I fix that.

Then I cloned these repos locally, in VSCode, opening a terminal in the repo root folder, and doing a ```git clone https...```. That adresse comes form DevOps, that will also generate for you. Which now means I need a KeyVault to store those credentials, which means I need an Azure subscription. Why not 1Password? I expect to need them accessible from code.

## Next steps

Provisioning scripts