---
layout: post
title:  "Basic ALM setup for Azure Stream Analytics - ALM 101 for ASA"
date:   2019-12-06 10:00:00 -0700
categories: ALM Azure ASA DevOps
---

# Basic ALM setup for Azure Stream Analytics - ASA-ALM-101

This is the first article of a series on enabling modern ALM practices for an Azure Stream Analytics project.

This is a work in progress, assumptions are being made, use at your own risk ;)

## Context

> Data streaming is awesome. Azure is awesome. Let's build a real time data pipeline using [Azure Stream Analytics](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-introduction) then! And now with the cloud and DevOps, let's do things right and "*Infrastructure as Code*" and "*CI/CD*" all the things!

Or so I told myself before struggling with implementation details so frustrating, I almost quit data engineering entirely. I reassured myself with the thought that this pain is often good evidence that work is being done on something that matters. Right?

To really benefit from what follows, one should have already played a little bit with **Azure Stream Analytics** (ASA). I expect it to be the case [directly in the Azure portal](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal). This article focus on moving out of the portal, and considering the [ALM](https://en.wikipedia.org/wiki/Application_lifecycle_management) (Application Life cycle Development) aspects of that transition. There is nothing below about the use case / scenario, the query or the pipeline itself.

### Pipelines?

As for everything ["DataOps"](https://medium.com/data-ops/why-do-dataops-8d4542eec3e5), we're going to have to consider two pipelines (at least). On one side, there's the data pipeline: the real time one that ASA enables. On the other side, there's the development pipeline: the meta one that takes our code from our [IDE](https://en.wikipedia.org/wiki/Integrated_development_environment) through staging and eventually production.

This schema from [DataKitchen](https://medium.com/data-ops) shows how data is moving left to right, and code is moving bottom to top:

![Two pipelines: data and code](https://miro.medium.com/proxy/1*Oo_SUpo729y9iuW14tUFwQ.png)

**Azure Stream Analytics** is the main engine of the **streaming pipeline**.

[Azure DevOps](https://dev.azure.com) is the main engine of the **development pipeline**, both with [Azure Repos](https://azure.microsoft.com/en-us/services/devops/repos/) and [Azure Pipelines](https://azure.microsoft.com/en-us/services/devops/pipelines/).

The plan is to set up a **local development environment** disconnected from Azure, either using [Visual Studio](https://visualstudio.microsoft.com/downloads/) or [Visual Studio Code](https://code.visualstudio.com/?wt.mc_id=vscom_downloads) (VSCode). There we'll create an ASA project and write a simple data pipeline that reads data from a sample input, **run a basic query locally** and output that to disk. We should also write some **scripts that can be used to provision** the infrastructure we will later need on Azure when time comes to deploy. We will check that project in **Azure Repos using Git**. From that we'll setup a **build** (we'll see what that means in our context) and **a release pipeline** to a staging environment.

Note that if you're looking for tips on how to test ASA jobs, I should do a first pass on unit testing at some point, and share some thoughts on my plans for integration testings. This won't be a deep coverage though, that may happen later hopefully.

### Components

Taking all that into account, we can start to assemble our puzzle:

![Making that schema ours](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_alm099.png?raw=true)

Of course an actual release pipeline will be more complex than a simple local development to staging flow. But if we get the basic wiring right, [we can add as many intermediary steps](https://docs.microsoft.com/en-us/azure/devops/pipelines/release/define-multistage-release-process?view=azure-devops) or targets as necessary (staging, integration, UAT, pre-prod, prod...).

Now if we want to do proper local development, we expect to be able to do local runs on local inputs and outputs. This will be enabled by the ASA extensions available for [VSCode](https://docs.microsoft.com/en-us/azure/stream-analytics/quick-create-vs-code) and [Visual Studio](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-vs). Using local inputs/outputs, with sample data, is a supported scenario both in [VSCode](https://docs.microsoft.com/en-us/azure/stream-analytics/vscode-local-run) and [Visual Studio](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-vs-tools-local-run).

Let's focus on a single IDE from now on, and pick [VSCode](https://code.visualstudio.com/). The main reason is that I've been enjoying it a lot lately, using it [remotely on the WSL](https://www.hanselman.com/blog/VisualStudioCodeRemoteDevelopmentMayChangeEverything.aspx). I don't expect things to be too different using Visual Studio instead.

The last thing we need to be aware of is that ASA jobs are deployed via [ARM templates](https://docs.microsoft.com/en-us/azure/azure-resource-manager/). ARM Templates are JSON files that describe a specific Azure resource, here an ASA job. ARM templates are usually 2 files: one that describe the resource itself, and one that holds credentials and values that should be parameterized. The ASA extension will handle those files for us, ["compiling"](https://docs.microsoft.com/en-us/azure/stream-analytics/quick-create-vs-code#compile-the-script) our ASA assets (query file, input/output files, config files) into the ARM template files.

Taking all that into account, at the end of our local setup we should get something like this:

![A representation of the assets discussed above](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_alm100_local.png?raw=true)

Once this is done, in the rest of the series we'll later discuss:

- The data pipeline itself running in Azure, with live input/outputs - including using Cosmos DB to debug a live stream
- The release pipeline from compiling ASA assets into an ARM template in VSCode to publishing that to the service via Azure Pipelines

Let's dig in.

## Developer setup

### Requirements

- Install [Git](https://git-scm.com/downloads)
- Install [Visual Studio Code](https://code.visualstudio.com/)
- Install the Azure Stream Analytics extension
  - Open **VSCode**
  - From **Extensions** on the left pane, search for **Stream Analytics** and select **Install** on the Azure Stream Analytics extension
    - Note that there are other prerequisites but VSCode should chime in if they are not installed

![ASA Extension in VSCode screen capture](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_almASAextension.png?raw=true)

### Project

Now let's create a project.

To access the commands made available from the ASA extensions, we need to use the [command palette](https://code.visualstudio.com/docs/getstarted/userinterface#_command-palette). Two ways to do that: ```CTRL+SHIFT+P``` or via the menu : ```View > Command Palette```. Once it's there, we can type ```ASA create``` and select the ```ASA: Create new project``` option.

![ASA charm: Create new project](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_almASAproject.png?raw=true)

We'll pick a name for our project (I left the default option, **ASATest1**), then a folder, and we should end up with the following workspace:

- **ASATest1** (project folder)
  - **Functions** ([Javascript](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-javascript-user-defined-functions) or [C#](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-edge-csharp-udf) user defined functions (UDF), we should not use that one anytime soon)
  - **Inputs** (where our input configuration files will live)
    - *input.json* : an empty input configuration file to get us started
  - **Outputs** (where our output configuration files will live)
    - *output.json* : an empty output configuration file to get us started
  - *asaproj.json* : the configuration file of our project
  - *ASATest1.**asaql*** : our job query
  - *JobConfig.json* : the configuration file of the job, once it'll be running live (scale, compatibility version, error policy...)

Now that we have that, we should be able to compile these assets into ARM Templates files. From the command palette again, let's do a ```ASA Compile Script```:

![ASA charm: Create new project](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_almASAcompile.png?raw=true)

Which will add the following sub-folder and file to the project:

- **ASATest1** (project folder)
  - **Deploy**
    - *ASATest1.JobTemplate.json* : the job definition - we should never edit that file directly, but instead change the source files and compile
    - *ASATest1.JobTemplate.parameters.json* : that parameters (connection strings...) - we should never edit that file either, we will use variables in the release pipeline to manage those values properly
  - ...

This will be what we leverage later when we first set up the release pipeline: manual compile step in VSCode, then pushing those files to our repo, which will trigger a build using the ARM template files.

Speaking of our repo, we should actually set Git up before anything else - well we should have set Git up before even creating the project, but here we are.

### Version control with Git

As mentioned before, we will be using **Azure DevOps** ([free account](https://dev.azure.com)), and in this section we'll focus on **Azure Repos**.

We won't go through the details, the plan is to first create a DevOps project, then a repo for our local project. Since I tend to reserve the base repo (the default one created with the DevOps project) for documentation, we'll create a new one specifically for our ASA project:

![Azure DevOps new repo](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_almDevOpsproject.png?raw=true)

Then:

![Azure DevOps new repo details](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_almDevOpsrepo.png?raw=true)

Once this is done we can grab the origin address from the repo landing page (if that page is not there for any reason, the same https link is available under the **Clone** option in the top right corner of the repo **Files** page)

![Azure DevOps new repo origin address](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_almDevOpsorigin.png?raw=true)

Now that we have the central repo setup, we can initialize the local one, and link it to the origin.

Back in VSCode, we'll open a **terminal** and do:

```BASH
git init
git remote add origin https://xxxx@dev.azure.com/yyyy/zzzz/_git/aaaa
```

The origin being the one Azure Repos gave us just above.

![VSCode terminal](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_almlocalrepo.png?raw=true)

Now we need to commit our local changes in the local repo first:

![VSCode commit to local repo](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_almlocalcommit.png?raw=true)

And then push them to origin to see them in Azure DevOps:

![VSCode commit to local repo](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_almlocalpush.png?raw=true)

Now if we go back to Azure DevOps, we should see our files there, including the ARM template files:

![VSCode commit to local repo](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_almDevOpswired.png?raw=true)

We can add a step to our development process: every time we change our query or ASA config files, we should compile, commit the changes to the local repo, then push those changes to the origin in Azure Repos.

![A representation of the workflow](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_alm100_localrepo.png?raw=true)

Now that this is done, let's get back to performing a local run.

### Local run

The [ASA documentation](https://docs.microsoft.com/en-us/azure/stream-analytics/visual-studio-code-local-run) makes a good tutorial for that part. It comes as a later step of a larger tutorial that has set up a live source a bit earlier, and use that live output to generate a sample one for the local run. We actually don't need that.

Let's create a new text file in VSCode, put data in it, and save it as CSV in our input folder:

```CSV
Column 1, Column 2
A,100
B,30
C,150
```

Then let's create an ASA input configuration file to match that local data set. We can do that either via the command palette (```ASA: Add Input```) or right clicking on the input folder. It's a **local input**, relevant to our main script (asaql). Once the configuration file is created, we can use the helpers in VSCode to have it point to our CSV file.

```JSON
{
    "InputAlias": "column1and2",
    "Type": "Data Stream",
    "Format": "Csv",
    "FilePath": "localdata.csv",
    "ScriptType": "InputMock"
}
```

Note that the name of the config file that was just created is prefixed by ```Local_...```, allowing us to have both a live and local configuration files for a single input alias. This will come in handy when we want to test a query on a source that exists, but still want to do 100% local runs.

In our query, we will now be able to use the ```InputAlias``` above in the **FROM** clause:

```SQL
SELECT
	*
INTO
	[YourOutputAlias]
FROM
	[column1and2]
```

Note that for local run, we don't really care about the output alias. The local engine will not honor the output config format that has been defined, and instead use a json file format.

We should now be able to start a local run, either from the command palette with ```ASA: Start local run```, or if the query is open, using the **Run locally** option available at the top of the screen. We will chose to use the local input, and if it's the first run, we will have to accept some conditions.

This should open a new window that displays our execution results:

![Local run result](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_almlocalrun.png?raw=true)

This was easy!

Each of these runs will generate a sub folder in the new ```LocalRunOutputs``` top folder. These contain the output file (json format) among other files. If this is very good to look back at past run results, this will quickly pollute our repository if we don't take care of it. A good way to address that is to add a ```.gitignore``` file in our project folder, and add ```/LocalRunOutputs``` to it, so it's excluded from the git process.

![The setup so far](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_alm101_local.png?raw=true)

Here we have it: a tight local development environment in VSCode, with local execution that requires no Azure resource. It is to be noted that local files are loaded all at once by the local run engine, so we will need to be mindful if we want to play with time logics in the query - we nay have to switch to a live stream of data for more advanced windowing scenario. That sounds logical but it needs to be recognized.

## Next steps

- Provisioning scripts, to create the infrastructure we will need in Azure to host our ASA job
- The data pipeline itself running in Azure, with live input/outputs - including using Cosmos DB to debug a live stream
- The release pipeline, from compiling ASA assets into an ARM template in VSCode to publishing that to the service via Azure Pipelines
