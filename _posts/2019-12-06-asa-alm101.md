---
layout: post
title:  "Basic ALM setup for Azure Stream Analytics - ALM 101 for ASA"
date:   2019-12-06 10:00:00 -0700
categories: ALM Azure ASA DevOps
---

# Basic ALM setup for Azure Stream Analytics - ASA-ALM-101

This is the first article of a series on enabling modern ALM practices for an Azure Stream Analytics project.

This is a work in progress, assumptions are being made, use at your own risk ;)

## Context

> Data streaming is awesome. Azure is awesome. Let's build a real time data pipeline using [Azure Stream Analytics](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-introduction) then! And now with the cloud and DevOps, let's do things right and "*Infrastructure as Code*" and "*CI/CD*" all the things!

Or so I told myself before struggling with implementation details so frustrating, I almost quit data engineering entirely. I reassured myself with the thought that this pain is often good evidence that work is being done on something that matters. Right?

To really benefit from what follows, one should have already played a little bit with **Azure Stream Analytics** (ASA). I expect it to be the case [directly in the Azure portal](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal). This article focus on moving out of the portal, and considering the [ALM](https://en.wikipedia.org/wiki/Application_lifecycle_management) (Application Life cycle Development) aspects of that transition. There is nothing below about the use case / scenario, the query or the pipeline itself.

### Pipelines?

As for everything ["DataOps"](https://medium.com/data-ops/why-do-dataops-8d4542eec3e5), we're going to have to consider two pipelines (at least). On one side, there's the data pipeline: the real time one that ASA enables. On the other side, there's the development pipeline: the meta one that takes our code from our [IDE](https://en.wikipedia.org/wiki/Integrated_development_environment) through staging and eventually production.

This schema from [DataKitchen](https://medium.com/data-ops) shows how data is moving left to right, and code is moving bottom to top:

![Two pipelines: data and code](https://miro.medium.com/proxy/1*Oo_SUpo729y9iuW14tUFwQ.png)

**Azure Stream Analytics** is the main engine of the **streaming pipeline**.

[Azure DevOps](https://dev.azure.com) is the main engine of the **development pipeline**, both with [Azure Repos](https://azure.microsoft.com/en-us/services/devops/repos/) and [Azure Pipelines](https://azure.microsoft.com/en-us/services/devops/pipelines/).

The plan is to set up a **local development environment** disconnected from Azure, either using [Visual Studio](https://visualstudio.microsoft.com/downloads/) or [Visual Studio Code](https://code.visualstudio.com/?wt.mc_id=vscom_downloads) (VSCode). There we'll create an ASA project and write a simple data pipeline that reads data from a sample input, **run a basic query locally** and output that to disk. We should also write some **scripts that can be used to provision** the infrastructure we will later need on Azure when time comes to deploy. We will check that project in **Azure Repos using Git**. From that we'll setup a **build** (we'll see what that means in our context) and **a release pipeline** to a staging environment.

Note that if you're looking for tips on how to test ASA jobs, I should do a first pass on unit testing at some point, and share some thoughts on my plans for integration testings. This won't be a deep coverage though, that may happen later hopefully.

### Components

Taking all that into account, we can start to assemble our puzzle:

![Making that schema ours](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_alm099.png?raw=true)

Of course an actual release pipeline will be more complex than a simple local development to staging flow. But if we get the basic wiring right, [we can add as many intermediary steps](https://docs.microsoft.com/en-us/azure/devops/pipelines/release/define-multistage-release-process?view=azure-devops) or targets as necessary (staging, integration, UAT, pre-prod, prod...).

Now if we want to do proper local development, we expect to be able to do local runs on local inputs and outputs. This will be enabled by the ASA extensions available for [VSCode](https://docs.microsoft.com/en-us/azure/stream-analytics/quick-create-vs-code) and [Visual Studio](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-vs). Using local inputs/outputs, with sample data, is a supported scenario both in [VSCode](https://docs.microsoft.com/en-us/azure/stream-analytics/vscode-local-run) and [Visual Studio](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-vs-tools-local-run).

Let's focus on a single IDE from now on, and pick [VSCode](https://code.visualstudio.com/). The main reason is that I've been enjoying it a lot lately, using it [remotely on the WSL](https://www.hanselman.com/blog/VisualStudioCodeRemoteDevelopmentMayChangeEverything.aspx). I don't expect things to be too different using Visual Studio instead.

The last thing we need to be aware of is that ASA jobs are deployed via [ARM templates](https://docs.microsoft.com/en-us/azure/azure-resource-manager/). ARM Templates are JSON files that describe a specific Azure resource, here an ASA job. ARM templates are usually 2 files: one that describe the resource itself, and one that holds credentials and values that should be parameterized. The ASA extension will handle those files for us, ["compiling"](https://docs.microsoft.com/en-us/azure/stream-analytics/quick-create-vs-code#compile-the-script) our ASA assets (query file, input/output files, config files) into the ARM template files.

Taking all that into account, at the end of our local setup we should get something like this:

![A representation of the assets discussed above](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_alm100_local.png?raw=true)

Once this is done, in the rest of the series we'll later discuss:

- The data pipeline itself running in Azure, with live input/outputs - including using Cosmos DB to debug a live stream
- The release pipeline from compiling ASA assets into an ARM template in VSCode to publishing that to the service via Azure Pipelines

Let's dig in.

## Developer setup

### Requirements

- Install [Visual Studio Code](https://code.visualstudio.com/)
- Install the Azure Stream Analytics extension
  - Open **VSCode**
  - From **Extensions** on the left pane, search for **Stream Analytics** and select **Install** on the Azure Stream Analytics extension
    - Note that there are other prerequisites but VSCode should chime in if they are not installed

![ASA Extension in VSCode screen capture](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_almASAextension.png?raw=true)

### Project

Now let's create a project.

To access the commands made available from the ASA extensions, we need to use the [command palette](https://code.visualstudio.com/docs/getstarted/userinterface#_command-palette). Two ways to do that: ```CTRL+SHIFT+P``` or via the menu : ```View > Command Palette```. Once it's there, we can type ```ASA create``` and select the ```ASA: Create new project``` option.

![ASA charm: Create new project](https://github.com/Fleid/fleid.github.io/blob/master/_posts/201912_asa_alm101/asa_almASAproject.png?raw=true)

### Input

To have fun in a streaming scenario, we need a streaming data source. It used to be painful to get that part set up, now it's trivial.

One of the easiest one is to leverage the [Raspberry Pi Azure IoT Online Simulator](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal#run-the-iot-simulator) via an [IoT Hub](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal#prepare-the-input-data). To my knowledge it needs to sit in an open browser tab, so another option is to build a quick Azure Function [triggered by a timer](https://docs.microsoft.com/en-us/azure/azure-functions/functions-create-scheduled-function), bound to an [Event Hub in output](https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-event-hubs#output). I should cover that in another article later, but the coding is [straightforward](https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-event-hubs#output---c-example).

That IoT Hub (or Event Hub) will **not** be part of our project. In a real life scenario, it's either provided by another service, or needs to be put in a separate project since its dependencies and its life cycle rhythm are both different than our main job.

### Output

This basic pipeline will get a basic primary output: a blob store. It's fitting since it doesn't care about schema drift (crafting a job against SQL is always a pain since it requires a table update every time the schema is updated. We can push data in any format, and we don't really care about reading it since we won't really do that anyway.

I'm currently experimenting with using Cosmos DB with container set up with a short TTL (time-to-live) to observe what's flying in the pipeline. It's also schema-on-read, it can scale with ASA, allows exactly-once delivery (more on that later), and provide a Data Explorer experience form the portal. It's not free, but it's cheap for the value it brings.

### Query

Here is the query I'm currently using, notice 2 patterns: audit / prod-flag to debug output.